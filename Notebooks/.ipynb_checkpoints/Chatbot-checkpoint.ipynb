{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bfc171-a484-4fd0-8a48-475bc3f593b7",
   "metadata": {},
   "source": [
    "## Import Libraries and Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b75b53-665a-4fa3-980a-584f512307b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout \n",
    "from keras.optimizers import SGD \n",
    "import random \n",
    "\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json \n",
    "import pickle \n",
    "\n",
    "intents_file = open(r'C:\\Users\\akobe\\OneDrive\\Desktop\\Lighthouse\\After\\Chatbot\\Data\\intents.json').read()\n",
    "intents = json.loads(intents_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de885353-e97b-4a89-8bb1-8e5650b5adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy==1.21.1\n",
    "#used different kernel to install different version of numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd3ae2-d74f-41e2-8c53-f508906ca2e0",
   "metadata": {},
   "source": [
    "## Preprocessing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45832a63-81d7-4285-9290-dae92186417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data \n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['!', '?', ',', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8a5e07-7ae3-4317-9e2f-cdfea9467ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hi', 'there'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hey'], 'greeting'), (['Hola'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Nice', 'chatting', 'to', 'you', ',', 'bye'], 'goodbye'), (['Till', 'next', 'time'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Awesome', ',', 'thanks'], 'thanks'), (['Thanks', 'for', 'helping', 'me'], 'thanks'), (['How', 'you', 'could', 'help', 'me', '?'], 'options'), (['What', 'you', 'can', 'do', '?'], 'options'), (['What', 'help', 'you', 'provide', '?'], 'options'), (['How', 'you', 'can', 'be', 'helpful', '?'], 'options'), (['What', 'support', 'is', 'offered'], 'options'), (['How', 'to', 'check', 'Adverse', 'drug', 'reaction', '?'], 'adverse_drug'), (['Open', 'adverse', 'drugs', 'module'], 'adverse_drug'), (['Give', 'me', 'a', 'list', 'of', 'drugs', 'causing', 'adverse', 'behavior'], 'adverse_drug'), (['List', 'all', 'drugs', 'suitable', 'for', 'patient', 'with', 'adverse', 'reaction'], 'adverse_drug'), (['Which', 'drugs', 'dont', 'have', 'adverse', 'reaction', '?'], 'adverse_drug'), (['Open', 'blood', 'pressure', 'module'], 'blood_pressure'), (['Task', 'related', 'to', 'blood', 'pressure'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'entry'], 'blood_pressure'), (['I', 'want', 'to', 'log', 'blood', 'pressure', 'results'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'management'], 'blood_pressure'), (['I', 'want', 'to', 'search', 'for', 'blood', 'pressure', 'result', 'history'], 'blood_pressure_search'), (['Blood', 'pressure', 'for', 'patient'], 'blood_pressure_search'), (['Load', 'patient', 'blood', 'pressure', 'result'], 'blood_pressure_search'), (['Show', 'blood', 'pressure', 'results', 'for', 'patient'], 'blood_pressure_search'), (['Find', 'blood', 'pressure', 'results', 'by', 'ID'], 'blood_pressure_search'), (['Find', 'me', 'a', 'pharmacy'], 'pharmacy_search'), (['Find', 'pharmacy'], 'pharmacy_search'), (['List', 'of', 'pharmacies', 'nearby'], 'pharmacy_search'), (['Locate', 'pharmacy'], 'pharmacy_search'), (['Search', 'pharmacy'], 'pharmacy_search'), (['Lookup', 'for', 'hospital'], 'hospital_search'), (['Searching', 'for', 'hospital', 'to', 'transfer', 'patient'], 'hospital_search'), (['I', 'want', 'to', 'search', 'hospital', 'data'], 'hospital_search'), (['Hospital', 'lookup', 'for', 'patient'], 'hospital_search'), (['Looking', 'up', 'hospital', 'details'], 'hospital_search')]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing data \n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']: \n",
    "        #tokenize each word \n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        words.extend(word)\n",
    "        #add documents in corpus \n",
    "        documents.append((word, intent['tag']))\n",
    "        #add to our classes list  \n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7df494c-f43a-4a86-b5eb-4e0a463159be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 documents\n",
      "9 classes ['greeting', 'goodbye', 'thanks', 'options', 'adverse_drug', 'blood_pressure', 'blood_pressure_search', 'pharmacy_search', 'hospital_search']\n",
      "87 unique lemmatized words [\"'s\", 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "#lemmatization \n",
    "# lemmatize and lower each wor and remove duplicates \n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "#sort classes \n",
    "calsses = sorted(list(set(classes)))\n",
    "\n",
    "#documents = combination between patterns and intents \n",
    "print(len(documents), 'documents')\n",
    "#classes = intents \n",
    "print(len(classes), 'classes', classes)\n",
    "#words = all words, vocabulary \n",
    "print(len(words), 'unique lemmatized words', words)\n",
    "\n",
    "#pickle.dump(words,open('words.pkl','wb'))\n",
    "#pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe8051-b498-4ac0-afdd-3048e3929646",
   "metadata": {},
   "source": [
    "## Create Training and Testing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d3cb3c-0fc5-4d53-a12a-cbc4ead216a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akobe\\AppData\\Local\\Temp\\ipykernel_4320\\3047850909.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "#create the training data \n",
    "training = []\n",
    "\n",
    "#create empty array for the output \n",
    "output_empty = [0]* len(classes)\n",
    "\n",
    "#training set, bag of words for every sentence \n",
    "for doc in documents: \n",
    "    #initializing bag of words \n",
    "    bag = []\n",
    "    #list of toeknized words for the pattern \n",
    "    word_patterns = doc[0]\n",
    "    \n",
    "    #lemmatize each word - create base wordm in attempt to represent related words \n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    \n",
    "    #create bag of words array with 1, if word is found in current pattern \n",
    "    for word in words: \n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    \n",
    "    #outpur is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "#shuffle the features and make a numpy array \n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "#create training and testing lists. X - patterns, Y-intents \n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "print(\"Training data is created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c4ff9-3f2e-46f3-b330-de92081e8f48",
   "metadata": {},
   "source": [
    "## Interacting with the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb280cc-4bb9-490b-b925-79c6435064c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
